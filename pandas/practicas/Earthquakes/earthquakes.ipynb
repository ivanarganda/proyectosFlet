{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07521c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "def getYears(df):\n",
    "    return df[\"Year\"].unique()\n",
    "\n",
    "def create_index(df, idx):\n",
    "    total_events = df.shape[0]\n",
    "    df[idx] = (\n",
    "        df[\"Year\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"Month\"].astype(str).str.zfill(2)\n",
    "        + \"_\"\n",
    "        + df[\"latitude\"].round(3).astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"longitude\"].round(3).astype(str)\n",
    "        + \"_\"\n",
    "        + [str(uuid.uuid4())[:8] for _ in range(total_events)]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def classify_earthquake(row):\n",
    "    try:\n",
    "        mag = float(row[\"magnitude\"])\n",
    "        depth = float(row[\"depth\"])\n",
    "        gap = float(row[\"gap\"])\n",
    "        tsunami = int(row[\"tsunami\"])\n",
    "    except:\n",
    "        return \"Light\"  # si hay error en datos, lo marcamos como leve\n",
    "\n",
    "    # Light → soft or depth seismics\n",
    "    if (mag < 5.2) or (mag < 5 and depth > 150):\n",
    "        return \"Light\"\n",
    "\n",
    "    # Medium → moderate seismics\n",
    "    elif (5.2 <= mag < 6.8 and depth <= 200 and tsunami == 0):\n",
    "        return \"Medium\"\n",
    "\n",
    "    # Tough → high magnitude or tsunami\n",
    "    elif (mag >= 6.8) or (depth < 50) or (tsunami == 1):\n",
    "        return \"Tough\"\n",
    "\n",
    "    else:\n",
    "        return \"Light\"  # Ensure that returning \"Light\"\n",
    "\n",
    "def display_values(ax):\n",
    "    total = sum([p.get_height() for p in ax.patches])\n",
    "    for patch in ax.patches:\n",
    "        height = patch.get_height()\n",
    "        percentage = (height / total) * 100 if total > 0 else 0\n",
    "        ax.text(\n",
    "            patch.get_x() + patch.get_width() / 2,\n",
    "            height + 1,\n",
    "            f\"{int(height)} ({percentage:.1f}%)\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=10\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "def full_info():\n",
    "    return print(\"\"\"#------ INFO ------#\n",
    "# | Column        | Meaning                          | Unit / Scale                                                | Description                                                                                             |\n",
    "# | ------------- | -------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "# | **magnitude** | Earthquake magnitude             | Richter or Moment Magnitude (Mw)                            | A measure of the energy released. Higher = stronger quake.                                              |\n",
    "# | **cdi**       | *Community Determined Intensity* | I–X scale (Modified Mercalli Intensity, average from users) | Based on public “Did You Feel It?” reports. Reflects **felt intensity**.                                |\n",
    "# | **mmi**       | *Modified Mercalli Intensity*    | I–X scale                                                   | Estimated shaking intensity from instruments, not people.                                               |\n",
    "# | **sig**       | *Significance*                   | Numeric (0–1000)                                            | Composite score used by USGS to rank event importance (depends on magnitude, felt area, reports, etc.). |\n",
    "# | **nst**       | *Number of Stations*             | Count                                                       | Number of seismic stations that detected and reported the event. More = higher accuracy.                |\n",
    "# | **dmin**      | *Minimum Distance*               | Degrees (~111 km per degree)                                | Distance from epicenter to the nearest seismic station. Smaller = better accuracy.                      |\n",
    "# | **gap**       | *Azimuthal Gap*                  | Degrees                                                     | Largest angle between stations around the epicenter. Smaller = better network coverage.                 |\n",
    "# | **depth**     | *Hypocenter Depth*               | Kilometers (km)                                             | How deep below the Earth’s surface the earthquake occurred.                                             |\n",
    "# | **latitude**  | Geographic coordinate            | Degrees                                                     | Latitude of the earthquake’s epicenter.                                                                 |\n",
    "# | **longitude** | Geographic coordinate            | Degrees                                                     | Longitude of the earthquake’s epicenter.                                                                |\n",
    "# | **Year**      | Year of occurrence               | Year (e.g. 2024)                                            | Extracted from the `time` field (or event timestamp).                                                   |\n",
    "# | **Month**     | Month of occurrence              | Month (1–12)                                                | Useful for temporal trend analysis.                                                                     |\n",
    "# | **tsunami**   | Tsunami alert flag               | 0 = No, 1 = Yes                                             | Whether the earthquake generated a tsunami.                                                             |\n",
    "\n",
    "# 💡 Example Interpretations\n",
    "# Example\tMeaning\n",
    "# magnitude = 6.2\tStrong earthquake\n",
    "# cdi = 7, mmi = 6.5\tVery strong shaking, felt widely\n",
    "# sig = 850\tSignificant event, possibly newsworthy\n",
    "# nst = 130, dmin = 0.05, gap = 40\tVery well-detected and precisely located\n",
    "# depth = 12.0\tShallow earthquake (more damaging potential)\n",
    "# tsunami = 1\tTsunami generated — typically near coastlines\"\"\")\n",
    "\n",
    "def load_file(path: str):\n",
    "    \"\"\"\n",
    "    Load a dataset from a local path or URL (including Google Drive).\n",
    "    Supports CSV, Excel, JSON, Parquet, Feather, and Pickle formats.\n",
    "    \"\"\"\n",
    "    # Map extensions to pandas functions\n",
    "    extensions = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"xlsx\": pd.read_excel,\n",
    "        \"xls\": pd.read_excel,\n",
    "        \"json\": pd.read_json,\n",
    "        \"parquet\": pd.read_parquet,\n",
    "        \"feather\": pd.read_feather,\n",
    "        \"pickle\": pd.read_pickle,\n",
    "    }\n",
    "\n",
    "    url_pattern = r\"^https?:\\/\\/\"\n",
    "\n",
    "    # If it's a URL\n",
    "    if re.match(url_pattern, path):\n",
    "        # Handle Google Drive links\n",
    "        if \"drive.google.com\" in path:\n",
    "            file_id = re.search(r\"/d/([a-zA-Z0-9_-]+)\", path)\n",
    "            if file_id:\n",
    "                path = f\"https://drive.google.com/uc?id={file_id.group(1)}\"\n",
    "\n",
    "        # Download content\n",
    "        response = requests.get(path)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Error {response.status_code} while accessing URL: {path}\")\n",
    "\n",
    "        # Guess file extension\n",
    "        extension_match = re.search(r\"\\.(\\w+)(?:\\?|$)\", path)\n",
    "        extension = extension_match.group(1).lower() if extension_match else \"csv\"\n",
    "\n",
    "        # Read file based on extension\n",
    "        if extension in [\"csv\", \"txt\"]:\n",
    "            df = pd.read_csv(io.StringIO(response.content.decode(\"utf-8\")))\n",
    "        elif extension in [\"xlsx\", \"xls\"]:\n",
    "            df = pd.read_excel(io.BytesIO(response.content))\n",
    "        elif extension == \"json\":\n",
    "            df = pd.read_json(io.StringIO(response.content.decode(\"utf-8\")))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported extension in URL: '{extension}'\")\n",
    "\n",
    "    # If it's a local file\n",
    "    else:\n",
    "        extension = path.split(\".\")[-1].lower()\n",
    "        if extension not in extensions:\n",
    "            raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "        df = extensions[extension](path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def replaceAllEmptyByNA(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all types of empty/invalid values in the entire DataFrame with pd.NA.\n",
    "    \"\"\"\n",
    "    # Convert columns to best dtype\n",
    "    df = df.convert_dtypes()\n",
    "\n",
    "    # Trim spaces in string columns\n",
    "    df = df.apply(lambda col: col.str.strip() if col.dtype == \"string\" else col)\n",
    "\n",
    "    # Replace common \"empty\" values\n",
    "    df = df.replace(r\"^\\s*$\", pd.NA, regex=True)  # \"\", \" \", \"   \"\n",
    "    df = df.replace({\"None\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_mode(col: pd.Series, as_string: bool = False):\n",
    "    \"\"\"\n",
    "    Returns the mode of a column safely.\n",
    "    - Ignores NA\n",
    "    - Returns first if multiple\n",
    "    - Returns None if empty\n",
    "    \"\"\"\n",
    "    col = col.dropna()\n",
    "    if col.empty:\n",
    "        return None\n",
    "\n",
    "    moda = col.mode()\n",
    "    if moda.empty:\n",
    "        return None\n",
    "\n",
    "    val = moda.iloc[0]\n",
    "    return str(val) if as_string else val\n",
    "\n",
    "\n",
    "def count_empty_values(col: pd.Series) -> int:\n",
    "    \"\"\"Count how many missing values (pd.NA / NaN) are in a column.\"\"\"\n",
    "    return col.isna().sum()\n",
    "\n",
    "\n",
    "def checkIfEmpty(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print a summary if the DataFrame is empty or not.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"⚠️ The DataFrame is empty.\")\n",
    "    else:\n",
    "        print(f\"✅ Data loaded successfully: {len(df)} rows, {len(df.columns)} columns.\")\n",
    "\n",
    "\n",
    "def countWhiteSpacesRows(field: pd.Series) -> int:\n",
    "    \"\"\"Count how many cells contain whitespace characters.\"\"\"\n",
    "    return field.apply(lambda x: any(ch.isspace() for ch in str(x)) if isinstance(x, str) else False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e3fe4",
   "metadata": {},
   "source": [
    "## Checking and data clean 😎🔎\n",
    "- Load file\n",
    "- Check null values\n",
    "- Have as a guiance a full map info\n",
    "- Ensure that replacing all empty data by NA to avoid errors after using this following functions: fillna, dropna, isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = load_file(\"earthquake_data_tsunami.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be72de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "count_empty_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ff2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = replaceAllEmptyByNA(df)\n",
    "\n",
    "df.nunique()\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf6a50",
   "metadata": {},
   "source": [
    "### Management and calculus ⚖️ 📶\n",
    "\n",
    "#### Management 📃\n",
    "\n",
    "Manage data and do calculus once previous stage is done. This is all about knowing behaviour and get used to it and what's more get on with data well.\n",
    "\n",
    "- To begin with, as there is not a column which makes each data as unique, we will create a index combined with this following fields\n",
    "  - Year\n",
    "  - Month\n",
    "  - latitude\n",
    "  - longitude\n",
    "\n",
    "Rarely, got to see something like this. It is more normal to create an index using timestamp. In this case, there is not a timestamp field so we decide to use this four fields. By far, they are closest to be each other unique. To grow into that, the random is the solution. It would turn out just like this: \n",
    "\n",
    "  - Field named \"id_event\" created by create_index function\n",
    "\n",
    "- Later on, as we need to handle evolution and temporaly calculus, we got to create a new column named event_date which contains Year and Month column parsed into datetime to use it as index.It will ensure that working resample well. \n",
    "- To end with, set global variables \"total_events\" and \"years\" which they will be useful for the workflow\n",
    " \n",
    "#### Basic and advanced calculus 🤓\n",
    "\n",
    "- Basic statistics about magnitude and depth columns\n",
    "- Top 10 earthquakes by magnitude\n",
    "- Calculate overall mean and standard deviation\n",
    "- Show earthquakes closest to the average (most typical magnitudes)\n",
    "- Show the most dispersed earthquakes (further from the average)\n",
    "- Detect and display real outliers with the interquartile range (IQR) rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0994bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is not a unique id so we create a id\n",
    "# First we parse month and year to string and latitude and longitude too\n",
    "import random\n",
    "\n",
    "total_events = df.shape[0]\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Total number of events\n",
    "total_events = df.shape[0]\n",
    "\n",
    "# Create unique ID by combining information and a short hash\n",
    "df = create_index(df, \"id_event\")\n",
    "# Generate date column combined by month and year\n",
    "\n",
    "df[\"event_date\"] = df[\"Year\"].astype(str)+\"/\"+df[\"Month\"].astype(str)\n",
    "\n",
    "df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], format=\"%Y/%m\")\n",
    "\n",
    "df = df.set_index(\"event_date\")\n",
    "\n",
    "years = getYears(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about magnitude and depth columns\n",
    "stats_data = {\n",
    "    'Metric': [\n",
    "        'Mean Magnitude',\n",
    "        'Median Magnitude',\n",
    "        'Mode Magnitude',\n",
    "        'Std Dev (Depth)',\n",
    "        'Correlation Magnitude-Depth'\n",
    "    ],\n",
    "    'Value': [\n",
    "        df[\"magnitude\"].mean(),\n",
    "        df[\"magnitude\"].median(),\n",
    "        get_mode(df[\"magnitude\"]),\n",
    "        df[\"depth\"].std(),\n",
    "        df[\"magnitude\"].corr(df[\"depth\"])\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 earthquakes by magnitude\n",
    "top10 = df.sort_values(\"magnitude\", ascending=False).head(10)\n",
    "display(top10[[\"magnitude\",\"depth\",\"gap\",\"sig\",\"tsunami\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall mean and standard deviation\n",
    "mean_mag = df[\"magnitude\"].mean()\n",
    "std_mag = df[\"magnitude\"].std()\n",
    "\n",
    "data_set = {\n",
    "    'Overall Mean Magnitude': mean_mag,\n",
    "    'Overall Std Dev Magnitude': std_mag\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(data_set, index=[0]))\n",
    "\n",
    "group_stats = df.groupby(\"tsunami\")[\"magnitude\"].agg([\"mean\",\"std\",\"min\",\"max\",\"median\"])\n",
    "display(group_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show earthquakes closest to the average (most typical magnitudes)\n",
    "df[\"dist_from_mean\"] = abs(df[\"magnitude\"] - mean_mag)\n",
    "closest_to_mean = df.nsmallest(10, \"dist_from_mean\")[[\"magnitude\",\"depth\",\"gap\",\"sig\",\"tsunami\"]]\n",
    "display(\"Earthquakes closest to the global average:\")\n",
    "display(closest_to_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c17dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the most dispersed earthquakes (further from the average)\n",
    "farthest_from_mean = df.nlargest(10, \"dist_from_mean\")[[\"magnitude\",\"depth\",\"gap\",\"sig\",\"tsunami\"]]\n",
    "display(\"Earthquakes farthest from the average (most atypical):\")\n",
    "display(farthest_from_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and display real outliers with the interquartile range (IQR) rule\n",
    "Q1 = df[\"magnitude\"].quantile(0.25)\n",
    "Q3 = df[\"magnitude\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Allowed range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df[\"magnitude\"] < lower_bound) | (df[\"magnitude\"] > upper_bound)]\n",
    "\n",
    "display(f\"Allowed range: {lower_bound:.2f} - {upper_bound:.2f}\")\n",
    "display(f\"Outliers detected: {outliers.shape[0]}\")\n",
    "\n",
    "display(outliers[[\"magnitude\",\"depth\",\"gap\",\"sig\",\"tsunami\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c7137",
   "metadata": {},
   "source": [
    "### Visualization 🎯 📈\n",
    "\n",
    "Definitely, this is the end part of dataflow. Let's represent all both implicit and explicit data through different kind of calculus.\n",
    "\n",
    "- Monthly evolution of magnitude: At first, it orders whatever year by input provided that it exists in dataset. Afterwards, it builds a dataframe filtered by year. To end with, it represents a basic plot displaying the evolution by month, but only for the year filtered. \n",
    "\n",
    "- Annual Magnitude Evolution: it represents a basic plot displaying the evolution of magnitudes by year\n",
    "\n",
    "- Magnitude mean by month: it represents a basic plot displaying the mean of magnitudes by month. At first, it orders whatever year by input provided that it exists in dataset. Afterwards, it builds a dataframe filtered by year. To end with respect to the representation. \n",
    "\n",
    "- Earthquake Classification (Magnitude + Depth + Gap + Tsunami): it represents all earthquakes over years. They are classificated not only by magnitude, but also by depth, gap and tsunami. It is worth bearing in mind that rating this kind of classifications because we know by far what the earthqueake most destroyes and keep count by next time we will have known or will know which will be the next earthquake either light, medium or tough through predictions or depth learning algorithms. \n",
    "\n",
    "- Distribution of Earthquakes by Year: it represents by percentage the distribution of earthqueakes over years through a pie chart.\n",
    "\n",
    "- Boxplot dispersion magnitude, depth and gap: it represents the dispersion and how set of data is dispersed over average through boxplot both magnitude, depth and gap over years. What's more it represents ooutliers which they are inusual. \n",
    "\n",
    "- Magnitude by Tsunami Presence: it is as same as the previous one but only by tsunami. It represents the dispersion and how set of data is dispersed over average through boxplot either no or yes tsunami and what's more it displays the outlier over years.\n",
    "\n",
    "- Distribution of magnitude according either not or yes thsunami presence: it represents the distribution f magnitude over years either not or yes tsunami presence. In this case, the reresentation is made by linear chart instead targeting a vertial line intersecting chart which it displays the mean.  \n",
    "\n",
    "- Correlation Matrix between Earthquake Variables: it represents all variables which one most near to average, disperse or outlier each other from earthquakes through heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8441a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly evolution of magnitude\n",
    "try:\n",
    "\n",
    "    year = int(input(f\"Type years {\",\".join(map(str, years ))}\"))\n",
    "\n",
    "    if year not in years:\n",
    "        raise Exception(\"Not found year in dataset\")\n",
    "\n",
    "    df_magnitude_year = df[df[\"Year\"] == year]\n",
    "\n",
    "    monthly_mag_count = df_magnitude_year[\"magnitude\"].resample(\"ME\").count()\n",
    "\n",
    "    monthly_mag_count.plot(\n",
    "        figsize=(10,5),\n",
    "        color=\"orange\",\n",
    "        title=f\"Monthly magnitude count evolution of {year}\",\n",
    "        ylabel=\"Mean magnitude\",\n",
    "        xlabel=\"Month\",\n",
    "    )\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    display(e)\n",
    "except NameError as e:\n",
    "    print(e)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual evolution of magnitude\n",
    "annual_counts = df[\"magnitude\"].resample(\"YE\").count()\n",
    "\n",
    "annual_counts.index = annual_counts.index.year # Access to year and only choose one\n",
    "annual_counts = annual_counts.sort_index() # sort index ascending\n",
    "\n",
    "annual_counts.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(10,5),\n",
    "    title=\"Annual evolution of magnitude events\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Earthquakes number\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude mean by month\n",
    "try:\n",
    "\n",
    "    year = int(input(f\"Type years {\",\".join(map(str, years ))}\"))\n",
    "\n",
    "    if year not in years:\n",
    "        raise Exception(\"Not found year in dataset\")\n",
    "\n",
    "    df_magnitude_year = df[df[\"Year\"] == year]\n",
    "\n",
    "    monthly_mag_mean = df_magnitude_year[\"magnitude\"].resample(\"ME\").mean()\n",
    "\n",
    "    monthly_mag_mean.plot(\n",
    "        figsize=(10,5),\n",
    "        color=\"orange\",\n",
    "        title=f\"Monthly magnitude mean of {year}\",\n",
    "        ylabel=\"Mean magnitude\",\n",
    "        xlabel=\"Month\",\n",
    "    )\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    display(e)\n",
    "except NameError as e:\n",
    "    print(e)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthquake Classification (Magnitude + Depth + Gap + Tsunami)\n",
    "\n",
    "df[\"magnitude\"] = pd.to_numeric(df[\"magnitude\"], errors=\"coerce\")\n",
    "df[\"depth\"] = pd.to_numeric(df[\"depth\"], errors=\"coerce\")\n",
    "df[\"gap\"] = pd.to_numeric(df[\"gap\"], errors=\"coerce\")\n",
    "df[\"tsunami\"] = pd.to_numeric(df[\"tsunami\"], errors=\"coerce\")\n",
    "\n",
    "df[\"rating_advanced\"] = df.apply(classify_earthquake, axis=1)\n",
    "\n",
    "display(df[\"magnitude\"].min())\n",
    "\n",
    "# Create a bar plot of earthquake ratings\n",
    "rating_order = [\"Light\", \"Medium\", \"Tough\"]\n",
    "rating_counts = df[\"rating_advanced\"].value_counts().reindex(rating_order)\n",
    "\n",
    "display(count_empty_values(df))\n",
    "\n",
    "# Create a bar plot with customized style\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = rating_counts.plot(kind=\"bar\", color=[\"lightblue\", \"orange\", \"red\"])\n",
    "\n",
    "plt.title(\"Earthquake Classification (Magnitude + Depth + Gap + Tsunami)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Number of Earthquakes\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# ✅ Add values on top of bars (with %)\n",
    "ax = display_values(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55435aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Earthquakes by Year\n",
    "total_earthquakes = len(df)\n",
    "earthquakes_by_year = df[\"magnitude\"].resample(\"YE\").count()\n",
    "percentages = (earthquakes_by_year / total_earthquakes) * 100\n",
    "\n",
    "labels = [f\"{int(year)} - {count} ({p:.1f}%)\" for year, count, p in zip(\n",
    "    earthquakes_by_year.index.year, earthquakes_by_year, percentages)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(earthquakes_by_year, labels=labels, startangle=90, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Distribution of Earthquakes by Year\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion and how set of data is dispersed over average through boxplot both magnitude, depth and gap over years\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "display(\"Boxplot dispersion magnitude, depth and gap\")\n",
    "sns.boxplot(y=df[\"magnitude\"], ax=axes[0], color=\"skyblue\")\n",
    "sns.boxplot(y=df[\"depth\"], ax=axes[1], color=\"lightcoral\")\n",
    "sns.boxplot(y=df[\"gap\"], ax=axes[2], color=\"violet\")\n",
    "axes[0].set_title(\"Magnitude\"); axes[1].set_title(\"Depth\"); axes[2].set_title(\"Gap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb147a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion and how set of data is dispersed over average through boxplot either no or yes tsunami and what's more it displays the outlier over years\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "colors = [\"lightgray\" if t==0 else \"red\" for t in sorted(df[\"tsunami\"].unique())]\n",
    "sns.boxplot(data=df, x=\"tsunami\", y=\"magnitude\", hue=\"tsunami\", legend=False, palette=colors, ax=ax)\n",
    "plt.axhline(mean_mag, color=\"blue\", linestyle=\"--\", label=f\"Mean ({mean_mag:.2f})\")\n",
    "plt.title(\"Magnitude by Tsunami Presence\")\n",
    "plt.xticks([0,1], [\"No\", \"Yes\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of magnitude according either not or yes thsunami presence\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(data=df[df[\"tsunami\"]==0][\"magnitude\"], fill=True, label=\"Sin tsunami\", color=\"gray\", alpha=0.5)\n",
    "sns.kdeplot(data=df[df[\"tsunami\"]==1][\"magnitude\"], fill=True, label=\"Con tsunami\", color=\"red\", alpha=0.5)\n",
    "plt.axvline(mean_mag, color=\"blue\", linestyle=\"--\", label=f\"Media global: {mean_mag:.2f}\")\n",
    "plt.title(\"Distribución de magnitudes según presencia de tsunami\")\n",
    "plt.xlabel(\"Magnitud\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b390c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations heatmap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "plt.figure(figsize=(10,6))\n",
    "colors = [\"#FFCA00\",\"#FF9D00\",\"#FF7B00\",\"#FF4C00\",\"#D15E00\",\"#FF0000\",\"#FF0084\",\"#FF0055\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom\", colors, N=256)\n",
    "sns.heatmap(df[[\"magnitude\",\"depth\",\"gap\",\"dmin\",\"nst\",\"sig\"]].corr(), annot=True, cmap=cmap)\n",
    "plt.title(\"Correlation Matrix between Earthquake Variables\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
